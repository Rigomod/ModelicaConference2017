<h2>Papers by Vincent Reinbold:</h2>
<p>
<b>Title:</b> <i> Building Parallel FMUs (or Matryoshka Co-Simulations) </i> <br />
<b>Authors:</b> <a href="../authors/author_81.html">Virginie Galtier</a>, <a href="../authors/author_118.html">Michel Ianotto</a>, <a href="../authors/author_42.html">Mathieu Caujolle</a>, <a href="../authors/author_48.html">Rémi Corniglion</a>, <a href="../authors/author_267.html">Jean-Philippe Tavella</a>, <a href="../authors/author_67.html">José Évora Gómez</a>, <a href="../authors/author_109.html">José Juan Hernández Cabrera</a>, <a href="../authors/author_227.html">Vincent Reinbold</a> and <a href="../authors/author_144.html">Enrique Kremers</a><br />
<b>Abstract:</b>The development of complex multi-domain and multi-physic systems, such as Smart Electric Grids, have given rise to new challenges in the simulation domain. These challenges concern the capability to  couple multiple domain specific simulators, and the FMI standard is an answer to this. But they also concern the scalability and the accuracy of the simulation within an heterogenous system. We propose and implement here the concept of a Matryoshka FMU, i.e.~a first of its kind FMU that encapsulates DACCOSIM -- our distributed and parallel master architecture -- and several FMUs it controls. The Matryoshka automatically adapts its internal time steps to ensure the required accuracy while it is controlled by an external FMU-compliant simulator. We present the JavaFMI tools and the DACCOSIM middleware used in the automatic building process of such Matryoshka FMUs.~This approach is then applied on a real-life Distributed Energy System scenario. In regards of the Modelica system simulated  in Dymola, improvements up to 250% in terms of computational performance are achieved while preserving the simulation accuracy and enhancing its integration capability.<br />
<b>Links:</b> <a href="../submissions/ecp17132663_GaltierIanottoCaujolleCorniglionTavellaEvoragomezHernandezcabreraReinboldKremers.pdf">Full paper</a></p>
<hr />
<p>
<b>Title:</b> <i> Scaling FMI-CS Based Multi-Simulation Beyond Thousand FMUs on Infiniband Cluster </i> <br />
<b>Authors:</b> <a href="../authors/author_288.html">Stephane Vialle</a>, <a href="../authors/author_267.html">Jean-Philippe Tavella</a>, <a href="../authors/author_51.html">Cherifa Dad</a>, <a href="../authors/author_47.html">Remi Corniglion</a>, <a href="../authors/author_42.html">Mathieu Caujolle</a> and <a href="../authors/author_227.html">Vincent Reinbold</a><br />
<b>Abstract:</b>In recent years, co-simulation has become an increasingly industrial tool to simulate Cyber Physical Systems including multi-physics and control, like smart electric grids, since it allows to involve different modeling tools within the same temporal simulation. The challenge now is to integrate in a single calculation scheme very numerous and intensely inter-connected models, and to do it without any loss in model accuracy. This will avoid neglecting fine phenomena or moving away from the basic principle of equation-based modeling.

Offering both a large number of computing cores and a large amount of distributed memory, multi-core PC clusters can address this key issue in order to achieve huge multi-simulations in acceptable time. This paper introduces all our efforts to parallelize and distribute our co-simulation environment based on the FMI for Co-Simulation standard (FMI-CS). At the end of 2016 we succeeded to scale beyond 1000 FMUs and 1000 computing cores on different PC-clusters, including the most recent HPC Infiniband-cluster available at EDF.<br />
<b>Links:</b> <a href="../submissions/ecp17132673_VialleTavellaDadCorniglionCaujolleReinbold.pdf">Full paper</a></p>